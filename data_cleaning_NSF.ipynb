{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import mannwhitneyu\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the manual word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "# Define your text\n",
    "text = \"This is a test sentence.\"\n",
    "\n",
    "# Path to punkt pickle file\n",
    "nltk_data_path = '/Users/ryanseely/nltk_data'\n",
    "punkt_path = os.path.join(nltk_data_path, 'tokenizers', 'punkt', 'english.pickle')\n",
    "\n",
    "# Load sentence tokenizer manually from file\n",
    "with open(punkt_path, 'rb') as f:\n",
    "    sentence_tokenizer = pickle.load(f)\n",
    "\n",
    "# Tokenize\n",
    "sentences = sentence_tokenizer.tokenize(text)\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "tokens = [word_tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Flatten token list\n",
    "flat_tokens = [token for sublist in tokens for token in sublist]\n",
    "print(flat_tokens)\n",
    "\n",
    "# WE HAVE TO MANUALLY CREATE word_tokenize()\n",
    "\n",
    "def word_tokenize_manual(text):\n",
    "    # Path to your punkt file\n",
    "    nltk_data_path = '/Users/ryanseely/nltk_data'\n",
    "    punkt_path = os.path.join(nltk_data_path, 'tokenizers', 'punkt', 'english.pickl‌​e')\n",
    "\n",
    "    # Load sentence tokenizer manually\n",
    "    with open(punkt_path, 'rb') as f:\n",
    "        sentence_tokenizer = pickle.load(f)\n",
    "\n",
    "    # Tokenize into sentences\n",
    "    sentences = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "    # Tokenize into words using TreebankWordTokenizer\n",
    "    word_tokenizer = TreebankWordTokenizer()\n",
    "    tokens = [word_tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # Flatten and return token list\n",
    "    return [token for sublist in tokens for token in sublist]\n",
    "\n",
    "def word_tokenize_manual(text):\n",
    "    sentences = sentence_tokenizer.tokenize(text)\n",
    "    return [token for sent in sentences for token in word_tokenizer.tokenize(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data\n",
    "\n",
    "Create a function that pulls in each award dataset and creates a pandas dataframe called `awards_data`\n",
    "\n",
    "- This functions also adds a `Year` variable with the year awarded for each award\n",
    "\n",
    "Then it creates a dataframe for each year called `awards_xxxx`, where the x's represent the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline that imports data from year of ranges from github and add a year string variable to each observation for each year, then makes each year its own dataframe\n",
    "\n",
    "def load_and_create_award_data(start_year=2016, end_year=2025):\n",
    "    base_url = \"https://raw.githubusercontent.com/ryanpseely/reu_nsf/main/awards_{}.csv\"\n",
    "    all_data = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        try:\n",
    "            url = base_url.format(year)\n",
    "            df = pd.read_csv(url, encoding='latin1')\n",
    "            df['Year'] = str(year)\n",
    "            all_data.append(df)\n",
    "            globals()[f\"awards_{year}\"] = df  # Assign as variable\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {year}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data loaded. Please check the URLs or years.\")\n",
    "\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_data = load_and_create_award_data(2016, 2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many awards each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year-wise Award Counts:\n",
      "2016: 1599\n",
      "2017: 1631\n",
      "2018: 1306\n",
      "2019: 1845\n",
      "2020: 1793\n",
      "2021: 1620\n",
      "2022: 1745\n",
      "2023: 1458\n",
      "2024: 1542\n",
      "2025: 798\n",
      "Total count from individual years: 15337\n",
      "Total count from all years combined: 15337\n"
     ]
    }
   ],
   "source": [
    "# Count the number of awards for each year\n",
    "award_counts = {\n",
    "    \"2016\": len(awards_2016),\n",
    "    \"2017\": len(awards_2017),\n",
    "    \"2018\": len(awards_2018),\n",
    "    \"2019\": len(awards_2019),\n",
    "    \"2020\": len(awards_2020),\n",
    "    \"2021\": len(awards_2021),\n",
    "    \"2022\": len(awards_2022),\n",
    "    \"2023\": len(awards_2023),\n",
    "    \"2024\": len(awards_2024),\n",
    "    \"2025\": len(awards_2025) \n",
    "}\n",
    "\n",
    "# Print the counts neatly\n",
    "print(\"Year-wise Award Counts:\")\n",
    "for year, count in award_counts.items():\n",
    "    print(f\"{year}: {count}\")\n",
    "\n",
    "# Check that yearly count matches with total count\n",
    "total_count = len(awards_data)\n",
    "\n",
    "sum_of_yearly_counts = (len(awards_2016) + len(awards_2017) + len(awards_2018) + len(awards_2019) + len(awards_2020) + len(awards_2021) + len(awards_2022) + len(awards_2023) + len(awards_2024) + len(awards_2025))\n",
    "print(f\"Total count from individual years: {sum_of_yearly_counts}\")\n",
    "print(f\"Total count from all years combined: {total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NSF Award Statement repeated in most of the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: 0\n",
      "2017: 0\n",
      "2018: 1053\n",
      "2019: 1722\n",
      "2020: 1726\n",
      "2021: 1592\n",
      "2022: 1727\n",
      "2023: 1455\n",
      "2024: 1539\n",
      "2025: 797\n",
      "2016 remaining: 0\n",
      "2017 remaining: 0\n",
      "2018 remaining: 0\n",
      "2019 remaining: 0\n",
      "2020 remaining: 0\n",
      "2021 remaining: 0\n",
      "2022 remaining: 0\n",
      "2023 remaining: 0\n",
      "2024 remaining: 1\n",
      "2025 remaining: 0\n"
     ]
    }
   ],
   "source": [
    "boilerplate = (\n",
    "    \"This award reflects NSF's statutory mission and has been deemed worthy of support \"\n",
    "    \"through evaluation using the Foundation's intellectual merit and broader impacts review criteria.\"\n",
    ")\n",
    "\n",
    "# Count occurrences across all years\n",
    "total_count = awards_data[\"Abstract\"].str.contains(boilerplate, na=False).sum()\n",
    "counts_by_year = (\n",
    "    awards_data[\"Abstract\"]\n",
    "    .str.contains(boilerplate, na=False)\n",
    "    .groupby(awards_data[\"Year\"])\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Count each year's occurrences\n",
    "for year, count in counts_by_year.items():\n",
    "    print(f\"{year}: {count}\")\n",
    "\n",
    "# remove the boilerplate text from the Abstract column\n",
    "awards_data[\"Abstract\"] = awards_data[\"Abstract\"].str.replace(boilerplate, \"\", regex=False)\n",
    "\n",
    "# count how many are remaining and print\n",
    "remaining_counts = (\n",
    "    awards_data[\"Abstract\"]\n",
    "    .str.contains(boilerplate, na=False)\n",
    "    .groupby(awards_data[\"Year\"])\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "for year, count in remaining_counts.items():\n",
    "    print(f\"{year} remaining: {count}\")\n",
    "\n",
    "# boilerplate removed from all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryanseely/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ryanseely/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ryanseely/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ryanseely/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ryanseely/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper to convert POS tags to WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build text processing pipeline - just for TF-IDF, i should not apply this on the entire dataset \n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "# ---- Text Cleaning Helpers ----\n",
    "def clean_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize_manual(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmatized = []\n",
    "\n",
    "    for word, tag in tagged:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, wn_tag) if wn_tag else word\n",
    "        if tag.startswith('N') and p.singular_noun(lemma):\n",
    "            lemma = p.singular_noun(lemma)\n",
    "        lemmatized.append(lemma)\n",
    "\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# ---- Full Preprocessing Function ----\n",
    "def preprocess_abstract_column(df, col='Abstract'):\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[col]).copy()\n",
    "    after = len(df)\n",
    "    df[col] = df[col].apply(lambda x: lemmatize_text(\n",
    "        collapse_whitespace(\n",
    "            remove_numbers(\n",
    "                remove_punctuation(\n",
    "                    clean_html(x)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "    return df, before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_data_cleaned, before, after = preprocess_abstract_column(awards_data, col='Abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_data_cleaned.to_csv(\"awards_data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (original row count): 15337\n",
      "After (remaining with abstract): 15290\n",
      "Dropped rows: 47\n"
     ]
    }
   ],
   "source": [
    "print(\"Before (original row count):\", before)\n",
    "print(\"After (remaining with abstract):\", after)\n",
    "print(\"Dropped rows:\", before - after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract 1: Earth Science on Volcanic Island ESVI be a new Research Experience for Undergraduate REU Site host by the Department of Geology and Geophysic School of Ocean and Earth Science and Technology at the University of Hawaii at Mânoa Oceanic island formation evolution and sustainability be unify theme that a group of undergraduate student will explore during an exciting week program The Hawaiian Island be of volcanic origin be windows into the Earth interior Built by magma that have ascend roughly a hundred kilometer to reach the surface the volcano subsequently experience deformation erosion flank collapse and eventually sink below sea level Oceanic island be also site of intense biological evolution and be ecological niche in a vast ocean The University of Hawaii be uniquely position to take advantage of these phenomenon and our new program will provide cuttingedge STEM research opportunity for motivated undergraduate student in particular traditionally underrepresented group such a Native Hawaiian and Pacific Islander as well a student enrol at undergraduate institution that provide few opportunity to participate in cuttingedge research During the first week student will engage in a series of introductory daylong crashcourse on quantitative analysi and modeling tool which will enhance their ability to conduct research During the next eight week student will work alongside individual worldclas scientist host mentor who will guide the student through a modest yet challenge research project during an eightweek intense collaboration Following the core theme of Earth science on volcanic island the student activity be expect to contribute to various field of research but more importantly to engage motivated undergraduate student in the proces of do science and to energize them for future study in STEMrelated endeavor The research goal of this REU be to advance the knowledge of the formation evolution and sustainability of ocean island Specifically we will examine the growth and subsequent deformation of volcanic island use stateoftheart tool and methodology from petrology geochemistry volcanology structural geology seismology and geophysic We will also study the hydrologic pathway for subaerial and submarine groundwater flow use geochemical tracer as well a the biogeochemistry of marine organism thrive in the proximity of volcanic island\n",
      "\n",
      "Abstract 2: REU Site Sustainable RIVER Remediating InVasives to Encourage Resilience This REU Site award to the University of South Dakota locate in Vermillion SD will support the training of student for week during the summer of REU student participate in the Sustainable RIVER project will examine the functioning and management of the Missouri River a a len through which to study complex interdisciplinary system Through individual research project with faculty mentor from USD multidisciplinary Missouri River Institute student will address the question of how invasive element in the Missouri River and it upland affect the sustainability of the river and the human who depend on it Student will also meet a a team weekly to integrate knowledge gain from the individual project to create a team project to address the question of how a more resilient Missouri River which meet the need of multiple stakeholder and sustain diverse function ecosystem can be cultivate It be anticipate that a total of thirty student will be train in the program Active recruitment effort will focus on Native American student student in sustainability program across the ME and student attend and year institution in South Dakota There will be an explicit focu in the Sustainable RIVER project on the critical pedagogy of place where student learn how to live sustainably in place that have be disrupt while learn to recognize and address the cause of such disruption The Sustainable RIVER project will create student scientist who will become leader in interdisciplinary research and leader in create a more sustainable society through their development of a multiperspective systemsthinking approach to understanding and address complex challenge A common webbased assessment tool use by all REU program fund by the Division of Biological Infrastructure Directorate for Biological Science will be use to determine the effectivenes of the training program Student will be track after the program in order to determine student career paths Student will be ask to respond to an automatic email send via the NSF reporting system More information about the program be available by visit httpSustainableRIVERorg or by contact the PI Dr Meghann Jarchow at MeghannJarchowusdedu This REU site be cofunded by the Division of Biological Infrastructure Directorate for Biological Science and the Division of Earth Science Directorate for Geoscience\n",
      "\n",
      "Abstract 3: This REU Site award to Boston College locate in Boston MA will support the training of student for week during the summer of This project be support by the Division of Biological Infrastructure DBI and Chemistry CHE Project will generally pair student from different discipline to collaborate on solve fundamental science problem with an impact on society These include understand the growth of neuron generate clean energy from water identify protein responsible for infectious disease develop nanostructure for the brain and making interface for quantum computation These project include mentor from the physic chemistry biology mathematic and psychology department The program will include training in user facility graduate school preparation and oralwritten communication Potential participant should submit an application electronically include a resume college transcript two letter of recommendation and indication of research interest career goal prior experience and preferred project Participant selection will be conduct by the PI and CoPI in consultation with faculty mentor Emphasi will be place on student interested in integrated science with an impact on society It be anticipate the program will train a total of primarily underrepresented minority and first generation college student from school with limited research opportunities The REU will provide training by technical staff in user facility scientific communication and will include student seminar with feedback and preparation for take the GRE Student will thus receive professional and scientific skill train include opportunity to present their research at professional conference Combined with networkng activity and suite living these experience will give student a sense of belong in STEM along with individual and scientific growth A common webbased assessment tool use by all REU program fund by the Division of Biological Infrastructure Directorate for Biological Science will be use to determine the effectivenes of the training program Participant will be track after the program in order to determine student career paths Student will be ask to respond to an automatic email send via the NSF reporting system More information about the program be available by visit httpreubcedu or by contact the PI Dr Burch at ksburchbcedu or the coPI Dr Lowery at lauralowerybcedu\n",
      "\n",
      "Abstract 4: This REU Site award to Kettering University locate in Flint MI will support the training of ten student for eight week during the summer of The REU Site focus on the use of plant and plant product in innovative scientific and engineering research and faculty from the biological chemical and physical science as well a math and engineering will be involve in mentor and conduct direct research with participant In addition to undertake research during the eightweek program student will participate in seminar and journal club include training in ethic and responsible conduct of research and field trip to other research institution Participant for the REU site will be widely solicit to include student enrol in school on the quartersystem and those who be from group traditionally underrepresented in science and engineering A committee of faculty member from Kettering University will select REU participant base on academic performance interest in research and phone interview It be anticipate that a total of student primarily from school with limited research opportunity will be train in the program The interdisciplinary approach employ in the REU research project to tackle realworld issue will help to foster the student ability to collaborate with person from across STEM field allow participant to good engage with and solve st century problem Student will learn how research be conduct and many will present the result of their work at scientific conference A common webbased assessment tool use by all REU program fund by the Division of Biological Infrastructure Directorate for Biological Science will be use to determine the effectivenes of the training program Student will be track after the program in order to determine student career paths Student will be ask to respond to an automatic email send via the NSF reporting system More information about the program be available by visit httpTBD or by contact the PI Dr Lihua Wang at lwangketteringedu or the coPI Dr Jame Cohen at jcohenketteringedu\n",
      "\n",
      "Abstract 5: Engineered system be become increasingly complex and a a result they be more difficult to design Effective design require quickly bring diverse expertise to bear on each iteration of the design These two goal faster design cycle and good crossdisciplinary integration be enable by modelbased concurrent design approach These approach bring multiple expert together into a single room to create a design in a series of short design session and they be widely use in certain field such a spacecraft design These approach have many prove advantage but also potentially serious drawback due to the level of decomposition they impose on the design problem For example many assumption be make in order to decompose the design task into module for each team member and these assumption may not be valid for all design problem This award support fundamental research to generate knowledge about how the fit between the technical decomposition the problem be solve and the organizational decomposition person and task impact the quality of the design proces The result will enable design organization to make good inform choice about how to deploy concurrent design approach such a when these tool apply and when they do not and how to use them well even when the technical and organizational decomposition be slightly mismatch The resultant gain in design efficiency would have implication for many sector of the economy that depend on engineering design and innovation In addition this research involve several discipline include aerospace and system engineering organization design and qualitative research method this approach will foster multidisciplinary research and engineering education The objective of this research be to generate knowledge about how the lack of fit between a design organization impose decomposition and the problem natural decomposition affect the speed and characteristic of the design proces The research will focus on understand the nature of fit develop a framework for measure it identify the type of problem that arise in the design proces due to this lack of fit and explain the mechanism by which they arise Current research in this area be limit by measurement challenge and limited empirical evidence This research will address these limitation through an empirical study of the concurrent design team at the National Aeronautic and Space Administration Jet Propulsion Laboratory The research will be conduct through an indepth observational proces study of multiple concurrent design sessions The design proces will be observe it key characteristic measure and the technical and organizational decomposition will be record so that the impact of the decomposition fit on the design proces characteristic can be evaluate and it driver explain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, abstract in enumerate(awards_data_cleaned['Abstract'][:5]):\n",
    "    print(f\"Abstract {i+1}:\", abstract)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'fellowships' appears 3 times.\n"
     ]
    }
   ],
   "source": [
    "# Search for text I don't want - plurals, br, etc.\n",
    "full_text = ' '.join(awards_data_cleaned['Abstract'].tolist())\n",
    "\n",
    "word_to_count = 'fellowships'\n",
    "count = full_text.lower().split().count(word_to_count.lower())\n",
    "\n",
    "print(f\"The word '{word_to_count}' appears {count} times.\")\n",
    "\n",
    "# There is still a problem with plurals. Fellowship appears 985 times, fellowships appears 3 times. This is an improvement however, from adding the plural remover to the lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 317063),\n",
       " ('and', 262396),\n",
       " ('of', 216169),\n",
       " ('to', 165399),\n",
       " ('be', 127324),\n",
       " ('in', 126783),\n",
       " ('a', 116758),\n",
       " ('will', 86929),\n",
       " ('this', 63383),\n",
       " ('for', 62808)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(' '.join(awards_data_cleaned['Abstract']).lower().split())\n",
    "word_counts.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
